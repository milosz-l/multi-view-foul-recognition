{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SoccerNet.Downloader import SoccerNetDownloader as SNdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySNdl = SNdl(LocalDirectory=\"data\")\n",
    "# mySNdl.downloadDataTask(task=\"mvfouls\", split=[\"train\",\"valid\",\"test\",\"challenge\"], password=\"s0cc3rn3t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.video import read_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agorski/Desktop/projects/experiments_zzsn/.venv/lib/python3.11/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "video, audio, annot = read_video(\"data/mvfouls/challenge/action_0/clip_0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class name to label index\n",
    "\n",
    "EVENT_DICTIONARY_action_class = {\"Tackling\":0,\"Standing tackling\":1,\"High leg\":2,\"Holding\":3,\"Pushing\":4,\n",
    "                        \"Elbowing\":5, \"Challenge\":6, \"Dive\":7, \"Dont know\":8}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_action_class = {0:\"Tackling\", 1:\"Standing tackling\", 2:\"High leg\", 3:\"Holding\", 4:\"Pushing\",\n",
    "                        5:\"Elbowing\", 6:\"Challenge\", 7:\"Dive\", 8:\"Dont know\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_offence_severity_class = {\"No offence\":0,\"Offence + No card\":1,\"Offence + Yellow card\":2,\"Offence + Red card\":3}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_offence_severity_class = {0:\"No offence\", 1:\"Offence + No card\", 2:\"Offence + Yellow card\", 3:\"Offence + Red card\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_offence_class = {\"Offence\":0,\"Between\":1,\"No Offence\":2, \"No offence\":2}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_offence_class = {0:\"Offence\", 1:\"Between\", 2:\"No Offence\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_severity_class = {\"1.0\":0,\"2.0\":1,\"3.0\":2,\"4.0\":3,\"5.0\":4}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_severity_class = {0:\"No card\", 1:\"Borderline No/Yellow\", 2:\"Yellow card\", 3:\"Borderline Yellow/Red\", 4:\"Red card\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_bodypart_class = {\"Upper body\":0,\"Under body\":1}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_bodypart_class = {0:\"Upper body\", 1:\"Under body\"}\n",
    "\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY = {'action_class':EVENT_DICTIONARY_action_class, 'offence_class': EVENT_DICTIONARY_offence_class, \n",
    "            'severity_class': EVENT_DICTIONARY_severity_class, 'bodypart_class': EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': EVENT_DICTIONARY_offence_severity_class}\n",
    "INVERSE_EVENT_DICTIONARY = {'action_class':INVERSE_EVENT_DICTIONARY_action_class, 'offence_class': INVERSE_EVENT_DICTIONARY_offence_class, \n",
    "            'severity_class': INVERSE_EVENT_DICTIONARY_severity_class, 'bodypart_class': INVERSE_EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': INVERSE_EVENT_DICTIONARY_offence_severity_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "# from config.classes import EVENT_DICTIONARY\n",
    "\n",
    "##############################################################\n",
    "#                                                            #\n",
    "#              DO NOT MAKE ANY CHANGES HERE                  #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "\n",
    "\n",
    "# Function to load the labels from the json file\n",
    "def label2vectormerge(folder_path, split, num_views):\n",
    "\tpath_annotations = os.path.join(folder_path, split)\n",
    "\tpath_annotations = os.path.join(path_annotations, \"annotations.json\") \n",
    "\n",
    "\tdictionary_action = EVENT_DICTIONARY['action_class']\n",
    "\n",
    "\tif os.path.exists(path_annotations):\n",
    "\t\twith open(path_annotations) as f:\n",
    "\t\t\ttrain_annotations_data = json.load(f)\n",
    "\telse:\n",
    "\t\tprint(\"PATH DOES NOT EXISTS\")\n",
    "\t\texit()\n",
    "\n",
    "\tnot_taking = []\n",
    "\n",
    "\tnum_classes_action = 8\n",
    "\tnum_classes_offence_severity = 4\n",
    "\n",
    "\tlabels_action = []\n",
    "\tlabels_offence_severity= []\n",
    "\tnumber_of_actions = []\n",
    "\n",
    "\ttotal_distribution = torch.zeros(num_classes_offence_severity, num_classes_action)\n",
    "\tdistribution_action = torch.zeros(1, num_classes_action)\n",
    "\tdistribution_offence_severity = torch.zeros(1, num_classes_offence_severity)\n",
    "\n",
    "\tfor actions in train_annotations_data['Actions']:\n",
    "\t\taction_class = train_annotations_data['Actions'][actions]['Action class']\n",
    "\t\toffence_class = train_annotations_data['Actions'][actions]['Offence']\n",
    "\t\tseverity_class = train_annotations_data['Actions'][actions]['Severity']\n",
    "\n",
    "\n",
    "\t\tif action_class == '' or action_class == 'Dont know':\n",
    "\t\t\tnot_taking.append(actions)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif (offence_class == '' or offence_class == 'Between') and action_class != 'Dive':\n",
    "\t\t\tnot_taking.append(actions)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif (severity_class == '' or severity_class == '2.0' or severity_class == '4.0') and action_class != 'Dive' and offence_class != 'No offence' and offence_class != 'No Offence':\n",
    "\t\t\tnot_taking.append(actions)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif offence_class == '' or offence_class == 'Between':\n",
    "\t\t\toffence_class = 'Offence'\n",
    "\n",
    "\t\tif severity_class == '' or severity_class == '2.0' or severity_class == '4.0':\n",
    "\t\t\tseverity_class = '1.0'\n",
    "\n",
    "\t\tif num_views == 1:\n",
    "\t\t\tfor i in range(len(train_annotations_data['Actions'][actions]['Clips'])):\n",
    "\t\t\t\tif offence_class == 'No Offence' or offence_class == 'No offence':\n",
    "\t\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][0] = 1\n",
    "\t\t\t\t\tdistribution_offence_severity[0][0] += 1\n",
    "\t\t\t\t\toff_index = 0\n",
    "\t\t\t\telif offence_class == 'Offence' and severity_class == '1.0':\n",
    "\t\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][1] = 1\n",
    "\t\t\t\t\tdistribution_offence_severity[0][1] += 1\n",
    "\t\t\t\t\toff_index = 1\n",
    "\t\t\t\telif offence_class == 'Offence' and severity_class == '3.0':\n",
    "\t\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][2] = 1\n",
    "\t\t\t\t\tdistribution_offence_severity[0][2] += 1\n",
    "\t\t\t\t\toff_index = 2\n",
    "\t\t\t\telif offence_class == 'Offence' and severity_class == '5.0':\n",
    "\t\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][3] = 1\n",
    "\t\t\t\t\tdistribution_offence_severity[0][3] += 1\n",
    "\t\t\t\t\toff_index = 3\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnot_taking.append(actions)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tlabels_action.append(torch.zeros(1, num_classes_action))\n",
    "\t\t\t\tlabels_action[len(labels_action)-1][0][dictionary_action[action_class]] = 1\n",
    "\t\t\t\tdistribution_action[0][dictionary_action[action_class]] += 1\n",
    "\t\t\t\ttotal_distribution[off_index][dictionary_action[action_class]] += 1\n",
    "\t\telse:\n",
    "\t\t\tif offence_class == 'No Offence' or offence_class == 'No offence':\n",
    "\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][0] = 1\n",
    "\t\t\t\tdistribution_offence_severity[0][0] += 1\n",
    "\t\t\t\tindex = 0\n",
    "\t\t\telif offence_class == 'Offence' and severity_class == '1.0':\n",
    "\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][1] = 1\n",
    "\t\t\t\tdistribution_offence_severity[0][1] += 1\n",
    "\t\t\t\tindex = 1\n",
    "\t\t\telif offence_class == 'Offence' and severity_class == '3.0':\n",
    "\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][2] = 1\n",
    "\t\t\t\tdistribution_offence_severity[0][2] += 1\n",
    "\t\t\t\tindex = 2\n",
    "\t\t\telif offence_class == 'Offence' and severity_class == '5.0':\n",
    "\t\t\t\tlabels_offence_severity.append(torch.zeros(1, num_classes_offence_severity))\n",
    "\t\t\t\tlabels_offence_severity[len(labels_offence_severity)-1][0][3] = 1\n",
    "\t\t\t\tdistribution_offence_severity[0][3] += 1\n",
    "\t\t\t\tindex = 3\n",
    "\t\t\telse:\n",
    "\t\t\t\tnot_taking.append(actions)\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tnumber_of_actions.append(actions)\n",
    "\t\t\tlabels_action.append(torch.zeros(1, num_classes_action))\n",
    "\t\t\tlabels_action[len(labels_action)-1][0][dictionary_action[action_class]] = 1\n",
    "\t\t\tdistribution_action[0][dictionary_action[action_class]] += 1\n",
    "\t\t\ttotal_distribution[index][dictionary_action[action_class]] += 1\n",
    "\n",
    "\treturn labels_offence_severity, labels_action, distribution_offence_severity[0], distribution_action[0], not_taking, number_of_actions\n",
    "\n",
    "\n",
    "# Function to load the path to the clips\n",
    "def clips2vectormerge(folder_path, split, num_views, not_taking):\n",
    "\n",
    "\tpath_clips = os.path.join(folder_path, split)\n",
    "\n",
    "\tif os.path.exists(path_clips):\n",
    "\t\tfolders = 0\n",
    "\n",
    "\t\tfor _, dirnames, _ in os.walk(path_clips):\n",
    "\t\t\tfolders += len(dirnames) \n",
    "\t\t\t\n",
    "\t\tclips = []\n",
    "\t\tfor i in range(folders):\n",
    "\t\t\tif str(i) in not_taking:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\tif num_views == 1:\n",
    "\t\t\t\tpath_clip = os.path.join(path_clips, \"action_\" + str(i))\n",
    "\t\t\t\tpath_clip_0 = os.path.join(path_clip, \"clip_0.mp4\")\n",
    "\t\t\t\tclips_all_view = []\n",
    "\t\t\t\tclips_all_view.append(path_clip_0)\n",
    "\t\t\t\tclips.append(clips_all_view)\n",
    "\t\t\t\tclips_all_view = []\n",
    "\t\t\t\tpath_clip_1 = os.path.join(path_clip, \"clip_1.mp4\")\n",
    "\t\t\t\tclips_all_view.append(path_clip_1)\n",
    "\t\t\t\tclips.append(clips_all_view)\n",
    "\t\t\t\tclips_all_view = []\n",
    "\n",
    "\t\t\t\tif os.path.exists(os.path.join(path_clip, \"clip_2.mp4\")):\n",
    "\t\t\t\t\tpath_clip_2 = os.path.join(path_clip, \"clip_2.mp4\")\n",
    "\t\t\t\t\tclips_all_view.append(path_clip_2)\n",
    "\t\t\t\t\tclips.append(clips_all_view)\n",
    "\t\t\t\t\tclips_all_view = []\n",
    "\n",
    "\t\t\t\tif os.path.exists(os.path.join(path_clip, \"clip_3.mp4\")):\n",
    "\t\t\t\t\tpath_clip_3 = os.path.join(path_clip, \"clip_3.mp4\")\n",
    "\t\t\t\t\tclips_all_view.append(path_clip_3)\n",
    "\t\t\t\t\tclips.append(clips_all_view)\n",
    "\t\t\t\t\tclips_all_view = []\n",
    "\t\t\telse:\n",
    "\t\t\t\tpath_clip = os.path.join(path_clips, \"action_\" + str(i))\n",
    "\t\t\t\tpath_clip_0 = os.path.join(path_clip, \"clip_0.mp4\")\n",
    "\t\t\t\tclips_all_view = []\n",
    "\t\t\t\tclips_all_view.append(path_clip_0)\n",
    "\t\t\t\tpath_clip_1 = os.path.join(path_clip, \"clip_1.mp4\")\n",
    "\t\t\t\tclips_all_view.append(path_clip_1)\n",
    "\n",
    "\t\t\t\tif os.path.exists(os.path.join(path_clip, \"clip_2.mp4\")):\n",
    "\t\t\t\t\tpath_clip_2 = os.path.join(path_clip, \"clip_2.mp4\")\n",
    "\t\t\t\t\tclips_all_view.append(path_clip_2)\n",
    "\n",
    "\t\t\t\tif os.path.exists(os.path.join(path_clip, \"clip_3.mp4\")):\n",
    "\t\t\t\t\tpath_clip_3 = os.path.join(path_clip, \"clip_3.mp4\")\n",
    "\t\t\t\t\tclips_all_view.append(path_clip_3)\n",
    "\t\t\t\tclips.append(clips_all_view)\n",
    "\n",
    "\t\treturn clips\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from random import random\n",
    "import torch\n",
    "import random\n",
    "# from data_loader import label2vectormerge, clips2vectormerge\n",
    "from torchvision.io.video import read_video\n",
    "\n",
    "\n",
    "class MultiViewDataset(Dataset):\n",
    "    def __init__(self, path, start, end, fps, split, num_views, transform=None, transform_model=None):\n",
    "\n",
    "        if split != 'Chall':\n",
    "            # To load the annotations\n",
    "            self.labels_offence_severity, self.labels_action, self.distribution_offence_severity,self.distribution_action, not_taking, self.number_of_actions = label2vectormerge(path, split, num_views)\n",
    "            self.clips = clips2vectormerge(path, split, num_views, not_taking)\n",
    "            self.distribution_offence_severity = torch.div(self.distribution_offence_severity, len(self.labels_offence_severity))\n",
    "            self.distribution_action = torch.div(self.distribution_action, len(self.labels_action))\n",
    "\n",
    "            self.weights_offence_severity = torch.div(1, self.distribution_offence_severity)\n",
    "            self.weights_action = torch.div(1, self.distribution_action)\n",
    "        else:\n",
    "            self.clips = clips2vectormerge(path, split, num_views, [])\n",
    "\n",
    "        # INFORMATION ABOUT SELF.LABELS_OFFENCE_SEVERITY\n",
    "        # self.labels_offence_severity => Tensor of size of the dataset. \n",
    "        # each element of self.labels_offence_severity is another tensor of size 4 (the number of classes) where the value is 1 if it is the class and 0 otherwise\n",
    "        # for example if it is not an offence, then the tensor is [1, 0, 0, 0]. \n",
    "\n",
    "        # INFORMATION ABOUT SELF.LABELS_ACTION\n",
    "        # self.labels_action => Tensor of size of the dataset. \n",
    "        # each element of self.labels_action is another tensor of size 8 (the number of classes) where the value is 1 if it is the class and 0 otherwise\n",
    "        # for example if the action is a tackling, then the tensor is [1, 0, 0, 0, 0, 0, 0, 0]. \n",
    "\n",
    "        # INFORMATION ABOUT SLEF.CLIPS\n",
    "        # self.clips => list of the size of the dataset\n",
    "        # each element of the list is another list of size of the number of views. The list contains the paths to all the views of that particular action.\n",
    "\n",
    "        # The offence_severity groundtruth of the i-th action in self.clips, is the i-th element in the self.labels_offence_severity tensor\n",
    "        # The type of action groundtruth of the i-th action in self.clips, is the i-th element in the self.labels_action tensor\n",
    "        \n",
    "        self.split = split\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.transform = transform\n",
    "        self.transform_model = transform_model\n",
    "        self.num_views = num_views\n",
    "\n",
    "        self.factor = (end - start) / (((end - start) / 25) * fps)\n",
    "\n",
    "        self.length = len(self.clips)\n",
    "        print(self.length)\n",
    "\n",
    "    def getDistribution(self):\n",
    "        return self.distribution_offence_severity, self.distribution_action, \n",
    "    def getWeights(self):\n",
    "        return self.weights_offence_severity, self.weights_action, \n",
    "\n",
    "\n",
    "    # RETURNS\n",
    "    #\n",
    "    # self.labels_offence_severity[index][0] => tensor of size 4. Example [1, 0, 0, 0] if the action is not an offence\n",
    "    # self.labels_action[index][0] => tensor of size 8.           Example [1, 0, 0, 0, 0, 0, 0, 0] if the type of action is a tackling\n",
    "    # videos => tensor of shape V, C, N, H, W with V = number of views, C = number of channels, N = the number of frames, H & W = height & width\n",
    "    # self.number_of_actions[index] => the id of the action\n",
    "    #\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        prev_views = []\n",
    "\n",
    "        for num_view in range(len(self.clips[index])):\n",
    "\n",
    "            index_view = num_view\n",
    "\n",
    "            if len(prev_views) == 2:\n",
    "                continue\n",
    "\n",
    "            # As we use a batch size > 1 during training, we always randomly select two views even if we have more than two views.\n",
    "            # As the batch size during validation and testing is 1, we can have 2, 3 or 4 views per action.\n",
    "            cont = True\n",
    "            if self.split == 'Train':\n",
    "                while cont:\n",
    "                    aux = random.randint(0,len(self.clips[index])-1)\n",
    "                    if aux not in prev_views:\n",
    "                        cont = False\n",
    "                index_view = aux\n",
    "                prev_views.append(index_view)\n",
    "\n",
    "\n",
    "            video, _, _ = read_video(self.clips[index][index_view], output_format=\"THWC\")\n",
    "            frames = video[self.start:self.end,:,:,:]\n",
    "\n",
    "            final_frames = None\n",
    "\n",
    "            for j in range(len(frames)):\n",
    "                if j%self.factor<1:\n",
    "                    if final_frames == None:\n",
    "                        final_frames = frames[j,:,:,:].unsqueeze(0)\n",
    "                    else:\n",
    "                        final_frames = torch.cat((final_frames, frames[j,:,:,:].unsqueeze(0)), 0)\n",
    "\n",
    "            final_frames = final_frames.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.transform != None:\n",
    "                final_frames = self.transform(final_frames)\n",
    "\n",
    "            final_frames = self.transform_model(final_frames)\n",
    "            final_frames = final_frames.permute(1, 0, 2, 3)\n",
    "            \n",
    "            if num_view == 0:\n",
    "                videos = final_frames.unsqueeze(0)\n",
    "            else:\n",
    "                final_frames = final_frames.unsqueeze(0)\n",
    "                videos = torch.cat((videos, final_frames), 0)\n",
    "\n",
    "        if self.num_views != 1 and self.num_views != 5:\n",
    "            videos = videos.squeeze()   \n",
    "\n",
    "        videos = videos.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        if self.split != 'Chall':\n",
    "            return self.labels_offence_severity[index][0], self.labels_action[index][0], videos, self.number_of_actions[index]\n",
    "        else:\n",
    "            return -1, -1, videos, str(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.video import R3D_18_Weights, MC3_18_Weights\n",
    "from torchvision.models.video import R2Plus1D_18_Weights, S3D_Weights\n",
    "from torchvision.models.video import MViT_V2_S_Weights, MViT_V1_B_Weights\n",
    "from torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights, mvit_v1_b, MViT_V1_B_Weights\n",
    "\n",
    "path = \"data/mvfouls\"\n",
    "start_frame = 0\n",
    "end_frame = 125\n",
    "fps = 25\n",
    "num_views = 5\n",
    "pre_model = \"s3d\"\n",
    "max_num_worker = 8\n",
    "batch_size = 2\n",
    "data_aug = False\n",
    "\n",
    "if data_aug == 'Yes':\n",
    "        transformAug = transforms.Compose([\n",
    "                                          transforms.RandomAffine(degrees=(0, 0), translate=(0.1, 0.1), scale=(0.9, 1)),\n",
    "                                          transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "                                          transforms.RandomRotation(degrees=5),\n",
    "                                          transforms.ColorJitter(brightness=0.5, saturation=0.5, contrast=0.5),\n",
    "                                          transforms.RandomHorizontalFlip()\n",
    "                                          ])\n",
    "else:\n",
    "    transformAug = None\n",
    "\n",
    "if pre_model == \"r3d_18\":\n",
    "    transforms_model = R3D_18_Weights.KINETICS400_V1.transforms()        \n",
    "elif pre_model == \"s3d\":\n",
    "    transforms_model = S3D_Weights.KINETICS400_V1.transforms()       \n",
    "elif pre_model == \"mc3_18\":\n",
    "    transforms_model = MC3_18_Weights.KINETICS400_V1.transforms()       \n",
    "elif pre_model == \"r2plus1d_18\":\n",
    "    transforms_model = R2Plus1D_18_Weights.KINETICS400_V1.transforms()\n",
    "elif pre_model == \"mvit_v2_s\":\n",
    "    transforms_model = MViT_V2_S_Weights.KINETICS400_V1.transforms()\n",
    "\n",
    "dataset_Train = MultiViewDataset(path=path, start=start_frame, end=10, fps=fps, split='train',\n",
    "            num_views = 2, transform=transformAug, transform_model=transforms_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_Train,\n",
    "            batch_size=batch_size, shuffle=True,\n",
    "            num_workers=max_num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([0., 1., 0., 0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([2, 3, 10, 224, 224])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch = dataset_Train[0]\n",
    "print(len(batch))\n",
    "print(batch[0])\n",
    "print('-' * 100)\n",
    "print(batch[1])\n",
    "print('-' * 100)\n",
    "print(batch[2].shape)\n",
    "print('-' * 100)\n",
    "print(batch[3])\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tensor(tensor, dim=1, squeeze=False):\n",
    "    \"\"\"\n",
    "    A function to reshape PyTorch tensor `tensor` along some dimension `dim` to the batch dimension 0 such that the tensor can be processed in parallel. \n",
    "    If `sqeeze`=True, the dimension `dim` will be removed completely, otherwise it will be of size=1. Check `unbatch_tensor()` for the reverese function.\n",
    "    \"\"\"\n",
    "    batch_size, dim_size = tensor.shape[0], tensor.shape[dim]\n",
    "    returned_size = list(tensor.shape)\n",
    "    returned_size[0] = batch_size * dim_size\n",
    "    returned_size[dim] = 1\n",
    "    if squeeze:\n",
    "        return tensor.transpose(0, dim).reshape(returned_size).squeeze_(dim)\n",
    "    else:\n",
    "        return tensor.transpose(0, dim).reshape(returned_size)\n",
    "\n",
    "\n",
    "def unbatch_tensor(tensor, batch_size, dim=1, unsqueeze=False):\n",
    "    \"\"\"\n",
    "    A function to chunk pytorch tensor `tensor` along the batch dimension 0 and concatenate the chuncks on dimension `dim` to recover from `batch_tensor()` function.\n",
    "    If `unsqueee`=True, it will add a dimension `dim` before the unbatching. \n",
    "    \"\"\"\n",
    "    fake_batch_size = tensor.shape[0]\n",
    "    nb_chunks = int(fake_batch_size / batch_size)\n",
    "    if unsqueeze:\n",
    "        return torch.cat(torch.chunk(tensor.unsqueeze_(dim), nb_chunks, dim=0), dim=dim).contiguous()\n",
    "    else:\n",
    "        return torch.cat(torch.chunk(tensor, nb_chunks, dim=0), dim=dim).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class WeightedAggregate(nn.Module):\n",
    "    def __init__(self,  model, feat_dim, lifting_net=nn.Sequential()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lifting_net = lifting_net\n",
    "        num_heads = 8\n",
    "        self.feature_dim = feat_dim\n",
    "\n",
    "        r1 = -1\n",
    "        r2 = 1\n",
    "        self.attention_weights = nn.Parameter((r1 - r2) * torch.rand(feat_dim, feat_dim) + r2)\n",
    "\n",
    "        self.normReLu = nn.Sequential(\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.ReLU()\n",
    "        )        \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "   \n",
    "\n",
    "\n",
    "    def forward(self, mvimages):\n",
    "        B, V, C, D, H, W = mvimages.shape # Batch, Views, Channel, Depth, Height, Width\n",
    "        aux = self.lifting_net(unbatch_tensor(self.model(batch_tensor(mvimages, dim=1, squeeze=True)), B, dim=1, unsqueeze=True))\n",
    "\n",
    "\n",
    "        ##################### VIEW ATTENTION #####################\n",
    "\n",
    "        # S = source length \n",
    "        # N = batch size\n",
    "        # E = embedding dimension\n",
    "        # L = target length\n",
    "\n",
    "        aux = torch.matmul(aux, self.attention_weights)\n",
    "        # Dimension S, E for two views (2,512)\n",
    "\n",
    "        # Dimension N, S, E\n",
    "        aux_t = aux.permute(0, 2, 1)\n",
    "\n",
    "        prod = torch.bmm(aux, aux_t)\n",
    "        relu_res = self.relu(prod)\n",
    "        \n",
    "        aux_sum = torch.sum(torch.reshape(relu_res, (B, V*V)).T, dim=0).unsqueeze(0)\n",
    "        final_attention_weights = torch.div(torch.reshape(relu_res, (B, V*V)).T, aux_sum.squeeze(0))\n",
    "        final_attention_weights = final_attention_weights.T\n",
    "\n",
    "        final_attention_weights = torch.reshape(final_attention_weights, (B, V, V))\n",
    "\n",
    "        final_attention_weights = torch.sum(final_attention_weights, 1)\n",
    "\n",
    "        output = torch.mul(aux.squeeze(), final_attention_weights.unsqueeze(-1))\n",
    "\n",
    "        output = torch.sum(output, 1)\n",
    "\n",
    "        return output.squeeze(), final_attention_weights\n",
    "\n",
    "\n",
    "class ViewMaxAggregate(nn.Module):\n",
    "    def __init__(self,  model, lifting_net=nn.Sequential()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lifting_net = lifting_net\n",
    "\n",
    "    def forward(self, mvimages):\n",
    "        B, V, C, D, H, W = mvimages.shape # Batch, Views, Channel, Depth, Height, Width\n",
    "        aux = self.lifting_net(unbatch_tensor(self.model(batch_tensor(mvimages, dim=1, squeeze=True)), B, dim=1, unsqueeze=True))\n",
    "        pooled_view = torch.max(aux, dim=1)[0]\n",
    "        return pooled_view.squeeze(), aux\n",
    "\n",
    "\n",
    "class ViewAvgAggregate(nn.Module):\n",
    "    def __init__(self,  model, lifting_net=nn.Sequential()):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lifting_net = lifting_net\n",
    "\n",
    "    def forward(self, mvimages):\n",
    "        B, V, C, D, H, W = mvimages.shape # Batch, Views, Channel, Depth, Height, Width\n",
    "        aux = self.lifting_net(unbatch_tensor(self.model(batch_tensor(mvimages, dim=1, squeeze=True)), B, dim=1, unsqueeze=True))\n",
    "        pooled_view = torch.mean(aux, dim=1)\n",
    "        return pooled_view.squeeze(), aux\n",
    "\n",
    "\n",
    "class MVAggregate(nn.Module):\n",
    "    def __init__(self,  model, agr_type=\"max\", feat_dim=400, lifting_net=nn.Sequential()):\n",
    "        super().__init__()\n",
    "        self.agr_type = agr_type\n",
    "\n",
    "        self.inter = nn.Sequential(\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.Linear(feat_dim, feat_dim),\n",
    "            nn.Linear(feat_dim, feat_dim),\n",
    "        )\n",
    "\n",
    "        self.fc_offence = nn.Sequential(\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.Linear(feat_dim, feat_dim),\n",
    "            nn.Linear(feat_dim, 4)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_action = nn.Sequential(\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.Linear(feat_dim, feat_dim),\n",
    "            nn.Linear(feat_dim, 8)\n",
    "        )\n",
    "\n",
    "        if self.agr_type == \"max\":\n",
    "            self.aggregation_model = ViewMaxAggregate(model=model, lifting_net=lifting_net)\n",
    "        elif self.agr_type == \"mean\":\n",
    "            self.aggregation_model = ViewAvgAggregate(model=model, lifting_net=lifting_net)\n",
    "        else:\n",
    "            self.aggregation_model = WeightedAggregate(model=model, feat_dim=feat_dim, lifting_net=lifting_net)\n",
    "\n",
    "    def forward(self, mvimages):\n",
    "\n",
    "        pooled_view, attention = self.aggregation_model(mvimages)\n",
    "\n",
    "        inter = self.inter(pooled_view)\n",
    "        pred_action = self.fc_action(inter)\n",
    "        pred_offence_severity = self.fc_offence(inter)\n",
    "\n",
    "        return pred_offence_severity, pred_action, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights, MC3_18_Weights, mc3_18\n",
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights, s3d, S3D_Weights\n",
    "from torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights, mvit_v1_b, MViT_V1_B_Weights\n",
    "\n",
    "\n",
    "\n",
    "class MVNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, net_name='r2plus1d_18', agr_type='max', lifting_net=torch.nn.Sequential()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net_name = net_name\n",
    "        self.agr_type = agr_type\n",
    "        self.lifting_net = lifting_net\n",
    "        \n",
    "        self.feat_dim = 512\n",
    "\n",
    "        if net_name == \"r3d_18\":\n",
    "            weights_model = R3D_18_Weights.DEFAULT\n",
    "            network = r3d_18(weights=weights_model)\n",
    "        elif net_name == \"s3d\":\n",
    "            weights_model = S3D_Weights.DEFAULT\n",
    "            network = s3d(weights=weights_model)\n",
    "            self.feat_dim = 400\n",
    "        elif net_name == \"mc3_18\":\n",
    "            weights_model = MC3_18_Weights.DEFAULT\n",
    "            network = mc3_18(weights=weights_model)\n",
    "        elif net_name == \"r2plus1d_18\":\n",
    "            weights_model = R2Plus1D_18_Weights.DEFAULT\n",
    "            network = r2plus1d_18(weights=weights_model)\n",
    "        elif net_name == \"mvit_v2_s\":\n",
    "            weights_model = MViT_V2_S_Weights.DEFAULT\n",
    "            network = mvit_v2_s(weights=weights_model)\n",
    "            self.feat_dim = 400\n",
    "        else:\n",
    "            weights_model = R2Plus1D_18_Weights.DEFAULT\n",
    "            network = r2plus1d_18(weights=weights_model)\n",
    "                \n",
    "        network.fc = torch.nn.Sequential()\n",
    "\n",
    "        self.mvnetwork = MVAggregate(\n",
    "            model=network,\n",
    "            agr_type=self.agr_type, \n",
    "            feat_dim=self.feat_dim, \n",
    "            lifting_net=self.lifting_net,\n",
    "        )\n",
    "\n",
    "    def forward(self, mvimages):\n",
    "        return self.mvnetwork(mvimages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MVNetwork(net_name='r2plus1d_18', agr_type='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "weight_decay=0.001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, \n",
    "                                    betas=(0.9, 0.999), eps=1e-07, \n",
    "                                    weight_decay=weight_decay, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.batch_tensor(tensor, dim=1, squeeze=False)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agorski/Desktop/projects/experiments_zzsn/.venv/lib/python3.11/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 3, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 4, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 3, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 3, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n",
      "torch.Size([1, 3, 3, 10, 224, 224])\n",
      "torch.Size([1, 2, 3, 10, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m weights_model \u001b[38;5;241m=\u001b[39m R2Plus1D_18_Weights\u001b[38;5;241m.\u001b[39mDEFAULT\n\u001b[1;32m      2\u001b[0m network \u001b[38;5;241m=\u001b[39m r2plus1d_18(weights\u001b[38;5;241m=\u001b[39mweights_model)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets_offence_severity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmvclips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_Train\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmvclips_cuda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmvclips\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmvclips_cuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m, in \u001b[0;36mMultiViewDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     85\u001b[0m     index_view \u001b[38;5;241m=\u001b[39m aux\n\u001b[1;32m     86\u001b[0m     prev_views\u001b[38;5;241m.\u001b[39mappend(index_view)\n\u001b[0;32m---> 89\u001b[0m video, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mread_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_view\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTHWC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m frames \u001b[38;5;241m=\u001b[39m video[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend,:,:,:]\n\u001b[1;32m     92\u001b[0m final_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/experiments_zzsn/.venv/lib/python3.11/site-packages/torchvision/io/video.py:292\u001b[0m, in \u001b[0;36mread_video\u001b[0;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[1;32m    289\u001b[0m audio_timebase \u001b[38;5;241m=\u001b[39m _video_opt\u001b[38;5;241m.\u001b[39mdefault_timebase\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m container:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39maudio:\n\u001b[1;32m    294\u001b[0m             audio_timebase \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39maudio[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtime_base\n",
      "File \u001b[0;32mav/container/core.pyx:420\u001b[0m, in \u001b[0;36mav.container.core.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/input.pyx:79\u001b[0m, in \u001b[0;36mav.container.input.InputContainer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/codec/context.pyx:30\u001b[0m, in \u001b[0;36mav.codec.context.wrap_codec_context\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights_model = R2Plus1D_18_Weights.DEFAULT\n",
    "network = r2plus1d_18(weights=weights_model).cuda()\n",
    "for targets_offence_severity, targets_action, mvclips, action in dataset_Train:\n",
    "    mvclips_cuda = mvclips.unsqueeze(0).cuda()\n",
    "    print(mvclips_cuda.shape)\n",
    "    continue\n",
    "    batched_mvclips = batch_tensor(mvclips_cuda, dim=1, squeeze=True)\n",
    "    enc_output = network(batched_mvclips)\n",
    "    print(enc_output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([ 4.0572e-02,  3.7395e-02, -4.4805e-03,  4.4332e-02, -8.0134e-03,\n",
      "         8.9692e-02,  4.4581e-02,  1.7988e-02,  7.9263e-02,  8.2089e-02,\n",
      "         8.1651e-02,  1.2441e-01,  4.1011e-02,  5.2756e-02,  4.0515e-02,\n",
      "         1.3831e-02,  4.9442e-02,  6.3419e-02,  6.5689e-03,  9.7634e-02,\n",
      "        -4.9032e-02, -3.1900e-02,  1.5789e-02,  4.9427e-02,  3.3140e-02,\n",
      "        -7.7997e-03,  5.6822e-02,  5.5623e-02,  7.0151e-02,  1.3241e-02,\n",
      "         9.7916e-02,  9.8353e-03,  5.2859e-02,  2.0470e-02,  2.2956e-02,\n",
      "         9.5345e-02,  1.0353e-02,  6.1648e-02,  6.0529e-02, -3.4429e-01,\n",
      "         5.7699e-03,  5.6428e-02,  6.1417e-02,  2.3188e-02,  2.3180e-02,\n",
      "         5.3957e-02,  9.3559e-02,  5.9049e-02,  2.6653e-02,  3.1645e-02,\n",
      "         1.1577e-02,  2.2128e-02, -7.9027e-02,  5.8854e-02,  1.2945e-02,\n",
      "        -8.2755e-03, -5.1337e-03,  7.8360e-02,  1.2926e-01,  6.0436e-02,\n",
      "         4.0017e-02,  2.4657e-02,  2.3390e-02,  4.3700e-02,  4.0267e-02,\n",
      "         6.1277e-03,  3.6907e-02,  4.3346e-03,  3.9551e-02,  3.5488e-02,\n",
      "         3.5999e-02,  1.0008e-01,  7.2663e-02,  1.0192e-03,  3.7579e-03,\n",
      "         4.3311e-02,  4.1027e-02,  4.3915e-02, -5.4001e-02,  2.6339e-02,\n",
      "         8.7711e-02,  1.9743e-02, -7.6806e-02,  1.0357e-01,  3.5402e-02,\n",
      "         6.6265e-02,  7.7870e-02,  1.0441e-01,  4.1199e-02,  2.0364e-02,\n",
      "         2.5841e-02,  6.6638e-02,  2.6391e-02,  1.3349e-02,  1.0268e-01,\n",
      "         5.9327e-02,  5.8260e-02,  1.0859e-02,  1.0213e-02,  2.3493e-02,\n",
      "         7.2444e-02,  4.3300e-02,  6.7632e-02,  5.0931e-02, -7.0171e-03,\n",
      "         7.5060e-02,  3.3341e-03,  5.7897e-04,  5.0398e-02,  7.9193e-02,\n",
      "         8.8047e-02,  1.8995e-02,  2.9315e-02,  3.9236e-02,  7.3010e-02,\n",
      "        -4.0739e-03,  6.5035e-02,  7.0583e-02,  3.5766e-02,  5.3188e-02,\n",
      "         2.0243e-02,  9.5058e-02,  4.4195e-02, -2.8878e-02,  5.8085e-02,\n",
      "         6.6622e-02,  5.6397e-02,  5.0904e-02,  7.2604e-02,  1.0723e-02,\n",
      "         8.8604e-03,  7.1630e-02,  5.7513e-02,  7.8674e-02,  6.1189e-02,\n",
      "         1.2582e-03, -5.7356e-03,  6.6646e-02,  5.5290e-02,  4.2303e-02,\n",
      "         4.5954e-02, -5.1998e-02,  3.2512e-02,  6.6042e-02,  2.5720e-02,\n",
      "         2.5197e-02,  4.5756e-02,  3.8171e-02, -9.0496e-02,  7.3812e-02,\n",
      "         1.3428e-02,  5.4477e-02,  1.2991e-01, -3.6761e-02,  4.5118e-02,\n",
      "        -8.6475e-03,  4.6827e-02,  5.0199e-02,  6.2151e-02,  3.6863e-02,\n",
      "         6.7642e-02,  3.3091e-02,  4.8058e-02,  2.3732e-02, -2.0814e-02,\n",
      "        -2.0810e-02,  2.0323e-02,  8.5911e-03,  1.6100e-02,  2.8694e-02,\n",
      "        -5.4668e-03,  1.5742e-02,  3.5707e-02,  6.4000e-02,  8.0783e-02,\n",
      "         9.0794e-02,  2.4598e-02,  3.4879e-02, -2.2461e-02,  1.8270e-02,\n",
      "         1.9635e-02,  3.4101e-02,  4.2842e-02,  3.6147e-02,  2.8452e-02,\n",
      "         2.4572e-02,  1.9398e-02, -3.0148e-03, -2.1908e-02,  2.3465e-02,\n",
      "         5.5357e-02,  3.4280e-02,  3.9806e-02, -2.2246e-02,  3.0710e-02,\n",
      "         3.8327e-02,  7.7497e-02,  4.3070e-02,  2.7650e-02,  6.4503e-03,\n",
      "         1.4056e-02,  7.2416e-02,  4.9152e-02,  4.3662e-02,  4.8249e-02,\n",
      "        -2.3729e-02,  3.7663e-02,  1.3101e-02, -3.8477e-02,  5.8665e-02,\n",
      "         3.6290e-02,  6.5366e-03,  5.7409e-02,  5.3917e-02,  4.2214e-02,\n",
      "        -4.6452e-02,  6.5026e-02,  7.4758e-02,  1.7358e-02,  6.8782e-03,\n",
      "         3.4637e-02,  4.5619e-02, -1.5444e-02,  4.5129e-02,  1.0449e-01,\n",
      "         4.1238e-02, -4.2082e-03,  6.5654e-02, -3.4289e-02,  7.6638e-02,\n",
      "         8.1493e-02, -1.8219e-03,  4.2772e-02,  3.6250e-02, -3.5929e-03,\n",
      "         7.9531e-02,  7.1761e-02,  1.9828e-02,  1.2948e-02,  5.2348e-02,\n",
      "        -3.9535e-02, -2.9751e-02,  3.5425e-02,  6.2219e-02,  4.5254e-02,\n",
      "         1.8837e-02,  2.4692e-02,  3.7807e-02,  6.9005e-02,  2.7795e-02,\n",
      "         5.6658e-03,  5.8004e-02,  3.3529e-03,  8.2329e-02,  4.7668e-02,\n",
      "         4.5633e-02,  6.5757e-02,  2.1268e-02,  5.3030e-02,  2.6327e-02,\n",
      "         7.7153e-02,  3.8439e-02,  6.8107e-03,  9.8486e-03,  8.8322e-02,\n",
      "         5.4960e-02,  5.5065e-02,  6.3426e-02,  3.2902e-02,  9.8610e-03,\n",
      "        -2.6394e-04,  5.4147e-02,  3.9817e-02,  1.0694e-02,  8.9917e-02,\n",
      "         4.6506e-03,  4.8360e-02,  3.8233e-03,  4.9362e-02,  5.1265e-02,\n",
      "         7.2209e-02,  2.6448e-02,  3.7168e-02,  3.5116e-02,  5.7408e-02,\n",
      "         7.6144e-02,  5.0973e-02, -7.7477e-02,  1.0824e-02,  1.9488e-02,\n",
      "         5.2408e-02,  7.8239e-02,  5.6982e-02,  5.5297e-02,  1.7970e-02,\n",
      "        -1.4512e-02,  4.8328e-02,  3.0521e-02,  1.4513e-02,  4.0645e-02,\n",
      "         5.6238e-02,  3.1272e-02,  6.6501e-02, -1.8672e-02,  5.6077e-02,\n",
      "        -8.1687e-03,  2.1763e-02,  3.7830e-02,  3.6432e-02,  2.7663e-02,\n",
      "        -5.6376e-03,  7.9211e-02, -2.9200e-02,  6.1292e-02, -1.5195e-02,\n",
      "         7.0210e-03,  7.7926e-02,  5.8296e-02,  4.3471e-02,  6.1824e-02,\n",
      "         2.3590e-02,  2.2913e-02,  6.5462e-02,  1.3581e-02,  5.2904e-02,\n",
      "         7.4246e-02,  4.1207e-02,  7.0519e-03,  1.2456e-02, -2.0663e-02,\n",
      "         2.2293e-03,  3.8935e-02, -3.3008e-02,  4.1501e-02,  3.9442e-02,\n",
      "         2.9375e-02,  6.4449e-03,  2.7558e-02,  7.1677e-02,  4.4777e-02,\n",
      "         4.1690e-02,  4.2248e-02,  6.5018e-02,  4.5200e-02,  1.1041e-01,\n",
      "         1.7538e-02,  4.9602e-02,  1.0717e-02,  2.0676e-02,  4.2741e-02,\n",
      "         2.3602e-02,  6.1359e-02, -1.2764e-02,  6.5435e-02,  4.5486e-02,\n",
      "         2.5627e-02, -3.3477e-02,  3.2150e-02,  5.7479e-02,  4.6351e-02,\n",
      "         9.4122e-02, -3.4446e-02,  3.5951e-02,  7.2908e-02,  9.8212e-03,\n",
      "         6.0824e-02, -2.1460e-02, -4.4096e-03, -5.3902e-02,  6.0982e-02,\n",
      "         3.7390e-02,  2.0857e-02,  4.0894e-02,  6.2548e-02,  4.7478e-02,\n",
      "         4.7561e-02,  4.5719e-02,  1.2105e-02,  3.2820e-02,  2.9686e-03,\n",
      "         6.4103e-02,  7.6224e-02,  5.3595e-02, -1.5261e-02, -2.6543e-03,\n",
      "         8.3887e-02, -1.1266e-02,  8.7203e-02,  4.9888e-02,  1.3820e-02,\n",
      "         7.0807e-02,  3.8961e-02,  9.9096e-02,  5.4482e-02,  1.1611e-01,\n",
      "         1.0508e-02,  1.0887e-02, -2.1292e-01,  6.9025e-02, -1.4064e-02,\n",
      "         3.3069e-02,  7.7340e-02,  2.0172e-02,  4.8950e-02,  5.1243e-02,\n",
      "        -4.6559e-03,  1.1196e-01,  4.1817e-02,  5.7443e-02,  1.9214e-02,\n",
      "         1.8421e-02,  1.2886e-01, -4.6300e-03, -1.3784e-02,  1.6320e-02,\n",
      "         3.9564e-02,  5.2539e-02,  2.6771e-02,  2.4698e-02, -2.5642e-03,\n",
      "         3.5694e-02,  7.9322e-02,  5.8973e-02,  8.4272e-02,  4.5653e-02,\n",
      "         3.7180e-02, -1.6841e-02,  7.9716e-02,  6.7423e-02,  4.9934e-02,\n",
      "         4.4249e-02,  4.5574e-02,  5.1008e-02,  2.0359e-02,  9.5213e-02,\n",
      "         9.1625e-02,  3.7564e-02,  1.1033e-01,  5.8975e-02,  4.2854e-02,\n",
      "         7.2575e-02, -3.3894e-03, -6.5789e-02,  9.2955e-03,  7.8842e-02,\n",
      "        -5.4025e-03, -2.3863e-04,  5.4407e-03,  6.3686e-02,  4.7053e-02,\n",
      "         4.1709e-02,  6.0633e-02,  3.8279e-02,  2.3066e-02,  9.1504e-02,\n",
      "         3.9846e-02,  2.1138e-02,  8.2112e-03,  5.0794e-02,  1.7443e-02,\n",
      "         8.2472e-02,  4.5396e-02, -2.7464e-02,  2.3564e-02, -5.4337e-02,\n",
      "         3.8239e-02,  1.8212e-02,  8.4683e-02,  4.0775e-02,  5.8740e-02,\n",
      "         3.8831e-02,  1.6266e-02,  2.4293e-02,  1.1904e-02,  3.5010e-02,\n",
      "        -2.2431e-04, -2.4061e-02,  6.0464e-02,  4.5224e-02,  6.4068e-02,\n",
      "         4.9575e-02,  3.1176e-02,  1.9540e-02,  3.6334e-03,  8.6343e-03,\n",
      "         5.3699e-02,  4.9646e-02,  4.5879e-02,  2.0690e-02,  7.8659e-02,\n",
      "         8.2801e-02,  1.0660e-02, -2.8238e-02,  5.2446e-02,  6.1112e-03,\n",
      "         9.5376e-02,  4.4550e-02, -3.0099e-03, -1.0229e-02,  4.1960e-02,\n",
      "         5.2393e-02,  1.3145e-01,  5.0002e-02,  4.1807e-02,  5.6780e-02,\n",
      "         6.1579e-02,  2.0276e-02,  4.9827e-02,  1.8846e-02, -1.1217e-01,\n",
      "         4.9693e-02,  4.9200e-04], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0115, -0.0374, -0.0633,  ...,  0.1555, -0.1337, -0.1201],\n",
      "        [ 0.1313, -0.1029,  0.2456,  ..., -0.1485, -0.0307,  0.3256],\n",
      "        [ 0.0433, -0.0475, -0.0590,  ..., -0.0631, -0.0998, -0.1322],\n",
      "        ...,\n",
      "        [-0.0349,  0.0753,  0.0219,  ...,  0.1509, -0.1073, -0.0969],\n",
      "        [-0.2679, -0.0583,  0.0427,  ..., -0.2356,  0.0761, -0.0990],\n",
      "        [ 0.0951,  0.0263,  0.3628,  ..., -0.0335, -0.0044, -0.0406]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.4759e-02,  6.5536e-02,  1.3831e-02, -3.6115e-02, -1.3978e-02,\n",
      "         7.3506e-02, -1.0801e-02, -6.3039e-02, -1.4708e-02, -2.6017e-02,\n",
      "         1.4107e-03,  4.7169e-02, -4.8925e-02, -4.7721e-03,  7.3454e-02,\n",
      "        -6.6267e-02,  3.0960e-02, -2.8626e-02, -1.0797e-02, -2.3459e-03,\n",
      "         1.6853e-02, -3.3265e-02, -4.2156e-02, -8.5119e-03,  1.3653e-02,\n",
      "        -3.9857e-02,  1.2220e-02, -3.1776e-02, -2.3627e-02,  3.5864e-02,\n",
      "        -3.7955e-02,  9.1639e-05, -4.0906e-02, -2.3534e-02,  8.4629e-03,\n",
      "        -5.3014e-02,  4.4915e-02, -2.1031e-02, -2.6162e-02, -1.7382e-02,\n",
      "         4.4254e-02, -6.9218e-02,  4.3552e-02, -2.2169e-02,  6.4974e-02,\n",
      "         3.2359e-02, -1.3782e-02, -4.0555e-02,  3.3927e-02,  4.6259e-02,\n",
      "         2.0489e-02, -1.8313e-02,  3.6251e-02, -2.2459e-02, -2.6796e-02,\n",
      "        -2.9703e-02, -2.8206e-02, -2.6429e-03, -3.7115e-02, -3.5162e-02,\n",
      "         2.8801e-02, -3.1964e-02, -8.0406e-02,  1.5677e-02, -2.3895e-02,\n",
      "         3.9128e-02, -6.3627e-02, -2.1608e-02,  6.0345e-03,  3.4988e-02,\n",
      "         3.1376e-02,  2.2619e-02, -5.7373e-02, -1.9516e-02,  2.9243e-04,\n",
      "        -1.1130e-03,  4.1654e-03, -2.8623e-03, -6.1637e-02,  6.4663e-02,\n",
      "        -2.2358e-02,  2.3954e-02, -6.0795e-02, -1.9790e-02,  1.7356e-02,\n",
      "         1.1647e-02, -1.8557e-02, -8.8671e-03, -2.7150e-02, -2.6609e-02,\n",
      "         2.9208e-02,  5.4144e-02, -7.4407e-03,  2.2439e-02,  2.7910e-02,\n",
      "        -4.2932e-02, -4.9899e-03,  3.4688e-02, -4.0702e-02, -5.7169e-03,\n",
      "         3.6367e-02,  1.9735e-02,  8.9982e-03,  5.7913e-02, -2.2319e-02,\n",
      "         5.8427e-02, -4.4237e-02,  5.3666e-02,  2.8755e-02,  9.9017e-03,\n",
      "        -7.1048e-02, -2.7760e-02,  5.4036e-02, -2.0680e-02,  1.2552e-02,\n",
      "         2.1648e-02,  2.2638e-02, -3.9553e-02,  2.5834e-02, -7.3927e-02,\n",
      "        -3.2244e-02, -8.6076e-03,  3.0284e-03, -1.1669e-02, -3.0118e-02,\n",
      "        -2.1008e-02,  1.4934e-02,  7.8284e-02, -5.7066e-05, -9.3413e-03,\n",
      "         3.7837e-02,  8.2010e-03, -4.8684e-03,  3.1933e-02,  1.1326e-02,\n",
      "        -7.4043e-02,  2.1603e-02, -2.2784e-02, -2.8550e-02,  4.8878e-04,\n",
      "         4.0790e-03, -3.0600e-02,  8.6575e-03,  4.5534e-02, -1.1520e-01,\n",
      "        -5.3811e-03, -3.3048e-02,  6.1405e-02,  1.2791e-02,  1.2973e-02,\n",
      "         2.2850e-02,  9.9568e-03,  2.6269e-02, -1.9203e-02,  1.8892e-02,\n",
      "        -3.2752e-02, -3.2652e-02,  2.1360e-03,  4.2132e-02,  2.6081e-03,\n",
      "        -1.6177e-02,  3.4067e-02,  2.9738e-02, -1.5230e-02,  2.2250e-02,\n",
      "        -1.2840e-02,  3.3405e-02,  4.5293e-02, -3.2550e-03,  3.4082e-02,\n",
      "        -3.7063e-02, -9.1142e-03, -2.7543e-02, -1.5579e-02, -1.8382e-02,\n",
      "         6.8569e-02, -8.6743e-03, -1.4381e-02, -1.2192e-02,  1.4995e-02,\n",
      "         7.5350e-03, -7.5369e-02, -5.3192e-03,  5.2465e-02, -4.9632e-02,\n",
      "        -7.4600e-03,  2.4078e-02, -8.1128e-03,  3.7783e-02, -4.0670e-02,\n",
      "        -2.1112e-02,  1.6653e-02, -2.2101e-02,  2.0618e-02, -1.0314e-02,\n",
      "        -2.5456e-02,  9.9928e-03, -2.9185e-02, -6.2119e-02,  5.5086e-02,\n",
      "        -5.8105e-03,  4.8830e-02, -2.4761e-02,  5.8810e-02,  2.0567e-03,\n",
      "        -4.1868e-02, -3.8554e-02,  5.3107e-02, -3.0670e-02,  6.9404e-02,\n",
      "         8.1688e-04, -1.8012e-02, -1.7016e-02,  4.5257e-03, -4.6622e-02,\n",
      "         1.5013e-03, -5.8342e-02, -1.0477e-02, -1.5160e-02,  2.3965e-02,\n",
      "         4.5877e-02, -2.4808e-02,  2.0819e-02, -1.3426e-02, -8.9643e-03,\n",
      "         3.3393e-02, -7.9466e-02,  2.7805e-02, -1.6857e-02,  3.5865e-02,\n",
      "        -2.3182e-02, -6.3184e-02,  4.6939e-02,  4.4335e-02,  1.9073e-03,\n",
      "         1.7839e-02,  5.7722e-02, -3.0942e-02, -1.9033e-02,  9.9703e-03,\n",
      "         7.4090e-02, -2.1793e-02,  4.8778e-02,  3.4300e-02,  1.5701e-03,\n",
      "        -1.7637e-02, -5.4146e-03, -4.6057e-03, -2.9012e-02,  6.4714e-03,\n",
      "         1.7298e-02,  3.8506e-02, -1.1755e-03,  3.9230e-02, -9.7950e-02,\n",
      "        -2.1734e-02,  4.0764e-02, -6.4404e-03,  8.4401e-02, -2.4924e-02,\n",
      "        -6.4706e-03, -3.1668e-02,  4.4219e-03,  2.1821e-02, -6.4200e-04,\n",
      "        -2.8556e-02,  6.0435e-02, -3.1732e-02, -2.1675e-02,  4.5649e-03,\n",
      "        -8.5875e-02, -5.2865e-03, -6.8078e-02,  1.6805e-03,  4.3320e-02,\n",
      "         3.4399e-02, -7.0213e-04,  2.4385e-02,  6.2576e-02,  7.4632e-02,\n",
      "        -4.8276e-02, -2.5132e-02,  1.4831e-02,  1.1793e-02, -1.4721e-02,\n",
      "         3.9641e-02, -1.0325e-02, -2.4783e-02,  3.0527e-02, -2.7166e-02,\n",
      "        -4.8349e-02,  1.1900e-02,  5.9801e-02, -2.1244e-02, -2.5334e-02,\n",
      "         2.7899e-02, -3.6184e-02, -1.2787e-02,  1.7662e-02, -1.9429e-02,\n",
      "        -6.8304e-03,  2.9922e-02,  1.7711e-02, -5.5158e-03,  3.1169e-02,\n",
      "         9.3842e-03,  7.6824e-02,  8.1488e-03,  3.6155e-02, -1.5794e-02,\n",
      "        -7.1703e-02, -1.3423e-02, -1.5293e-02, -1.6551e-02,  7.8969e-03,\n",
      "         1.8355e-03,  5.9795e-02,  2.5221e-02,  1.3801e-02, -2.0591e-02,\n",
      "         3.7563e-02,  8.0934e-03, -1.6323e-03, -2.1196e-02, -1.1252e-02,\n",
      "         7.2256e-03, -1.7493e-03,  1.7461e-02,  3.1061e-02,  8.4053e-03,\n",
      "         3.6198e-03, -2.1617e-02,  1.6999e-02,  8.8916e-02,  4.8006e-02,\n",
      "        -7.7105e-02, -1.8053e-02, -2.4881e-02,  7.3097e-03, -5.1907e-02,\n",
      "        -1.6175e-02, -3.2683e-02, -5.5530e-02, -4.9351e-02, -5.6711e-02,\n",
      "        -2.3844e-04,  3.3855e-02, -2.6434e-02, -6.9253e-03,  1.3733e-02,\n",
      "        -1.4125e-02, -4.7036e-02, -4.7910e-03,  1.2318e-02,  2.4003e-02,\n",
      "         1.0349e-01,  3.1119e-02,  1.3072e-02, -6.6319e-03, -2.5189e-02,\n",
      "        -2.7994e-03,  3.3530e-02, -2.9787e-02, -4.2566e-02, -1.1602e-02,\n",
      "         2.5600e-02, -8.2467e-03, -1.8832e-03,  4.9665e-04,  2.3403e-03,\n",
      "         4.1105e-02,  3.6507e-02, -4.8934e-02,  3.0447e-02, -1.2825e-02,\n",
      "        -4.5496e-02, -3.4244e-02,  5.6959e-02, -1.1232e-02,  6.1870e-02,\n",
      "        -2.4468e-03, -3.0824e-02,  9.7102e-02, -1.4393e-02, -2.7556e-02,\n",
      "         1.7236e-02,  2.2939e-03, -2.7122e-02, -7.9306e-03,  7.1140e-02,\n",
      "        -1.3909e-02,  3.6735e-02, -1.6489e-03, -7.5003e-02,  2.3486e-02,\n",
      "        -3.0240e-02, -2.7440e-03, -6.3548e-04,  6.0010e-02,  6.6809e-02],\n",
      "       device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(network.parameters())[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
